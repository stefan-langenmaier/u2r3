\section{MEMA-Prinzip}
Die Idee des MEMA-Prinzip stammt aus der Diplomarbeit von Timo Weithöhner \cite{Weithoehner}. Es ermöglicht die Abspeicherung einer Ontolgie in eine Datenbank mit einem festen Satz von Tabellen. Dies vereinfacht es Regeln in SQL zu erstellen, da diese vorformuliert werden können.

Das ursprüngliche MEMA-Prinzip von Timo Weithöhner ist um folgende Punkte erweitert.
Es wurde erstmal auf den erweiterten Sprachschatz von OWL2 RL angepasst. Was aber besonders zu erwähnen ist sind die Relationen \emph{list} und \emph{history} \ref{relations-list-history}. Sie speichern keine Axiome wie alle anderen. Die Relation list dient als Hilfstruktur für Axiome, die eine variable Anzahl von Elementen abspeichern. Die Relation history wird benutzt, um den Abhängigkeitsverlauf beim inferieren von Fakten speichern zu können. Damit ist es möglich Ableitungen gezielt wieder rückgängig zu machen. Deswegen sind auch alle anderen Relationen mit einer \emph{id} Spalte ausgestattet, die Axiome bzw. komplexe Unterausdrücke eindeutig identifiziert.

Allgemein wurde bei der Erstellung der Tabellenstruktur darauf geachtet, das die Formulierung der Regeln in SQL vereinfacht wird. So ist im Normalfall die Ergebnismenge einer Regel dadurch zu erhalten, in dem man die beteiligten Relationen miteinander joinet auf den Variablen, die in den Regeln angegeben sind.

\input{db-schema}

\section{Optimierungen für das MEMA-Prinzip}

Im Paper Classifying $\mathcal{ELH}$ Ontologies in SQL Databases \cite{Delaitre2009} sind interessante Ansätze für Verbesserungen.

Zum einen wird eine Normalisierung vorgeschlagen. Diese ist über zwei Wege möglich:
\begin{itemize}
  \item Vereinheitlichung von strukturell gleichen Ausdrücken. Das bedeutet, das wenn die selben Ausdrücke mehrmals in einer Ontologie vorkommen, z.B. als Teile in einem komplexeren Ausdruck, werden alle diese gleichen Unterausdrücke nur einmal abgespeichert. Das hat den Vorteil, dass es den Speicherplatz verringert und über den selben Ausdruck nicht mehrmals geschloßen wird. Dies ist mit der OWLAPI sogar relativ leicht möglich, da sie schon von selbst strukturelle gleiche Ausdrücke zusammenfasst. Allerdings wurde diese Optimierung nicht umgesetzt, den für die Möglichkeit abgeleitete Fakten wieder zu löschen muss der Weg wie Fakten entstanden sind eindeutig sein. Hier würde man aber Probleme verursachen. Diese Optimierung ist grundsätzlich nicht ausgeschloßen. Für den Fall, dass man nichts löschen müsste könnte sie direkt umgesetzt werden. Für den Fall das man etwas löschen will, müsste man etwas geschickter Vorgehen. Auf diese Optimierung wurde auf Grund ihrer Komplexität in diesem Fall verzichtet.
  \item Umwandlung gewisser Konstrukte. Es wird außerdem vorgeschlagen manche Konstrukte umzuwandeln, so z.B. $A \sqsubseteq B \land C \Rightarrow A \sqsubseteq B, A \sqsubseteq C$. Damit könnte man sich manche Regeln sparen. Inwieferen dies im RL Fragment nützlich ist, da hier doch recht viele Konstrukte aus OWL2 zu gelassen sind und welche Regeln man damit wirklich einsparen kann ist eine offene Frage.
\end{itemize}


Desweitern wird beschrieben, wie sie die Leistung beim Laden der Ontologie steigern. Hier werden ebenfalls zwei Tricks verwendet:
\begin{itemize}
  \item Zum einen wird die Ontologie \emph{on-the-fly} in die Datenbank geschrieben, d.h. während die Ontologie gelesen wird wird sie auch parallel in die Datenbank geschrieben. Das hat den Vorteil das sie nicht zweimal durchlaufen werden muss. Die OWLAPI lässt zwar auch die Serialisierung direkt in eine Datenbank zu. Das Laden der Ontologie in die OWLAPI und den Schlussfolgerer ist aus den folgenden Gründen entkoppelt. Zum einen kann es sein, dass man die OWLAPI ersteinmal läd und erst später den Schlussfolgerer dazu startet. So ist zumindest der normale Ablauf. Hier würde ein solche Optimierung also nichts bringen. Zweitens müsste man sich bei einer direkten Serialisierung um alle Konstrukte in einer Ontologie kümmern und nicht nur um die, die man zum ableiten verwendet. Es würde also den Schlussfolgerer komplexer in seinem Aufbau machen.
  \item Es werden Konstrukte immer in eine \emph{in-memory} Relation eingefügt und dann in größeren Blöcken auf einmal rausgeschrieben. Ob diese Optimierung wirklich soviel bringt ist fraglich, da ein gutes Datenbanksystem ein solches Caching von alleine betreiben sollte. Außerdem liegt der Overhead dabei vermutlich eher beim parsen der vielen INSERT Statements. Trotzdem könnte man diese Optimierung in betracht ziehen. Sie wurde ebenfalls aus Komplexitätsgrunden außer Acht gelassen.
\end{itemize}

\subsection{Andere Serialisierungsmöglichkeiten}

TEXT

http://owldb.sf.net

Zwei sehr interessante Papers. Eine allerdings noch für die alte OWLAPI v2. Beide sind eher für Materialierung als für Schlussfolgern gedacht.