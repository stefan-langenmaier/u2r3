\section{MEMA-Prinzip}
\label{abschnitt-mema-prinzip}
Die Idee des MEMA-Prinzips stammt aus der Diplomarbeit von Timo Weithöhner \cite{Weithoehner2003}. Es ermöglicht die Abspeicherung einer Ontologie in eine Datenbank mit einem festen Satz von Tabellen. Dies vereinfacht es Regeln in SQL zu erstellen, da diese vorformuliert werden können.

Diese Diplomarbeit hat aufgezeigt, das die Umwandlung und Abspeicherung einer Ontologie in einen fixen Satz von Relationen möglich ist, so dass danach eine effiziente Schlussfolgerung durchgeführt werden kann.

Das ursprüngliche MEMA-Prinzip von Timo Weithöhner ist um folgende Punkte erweitert.
Es wurde erstmal auf den erweiterten Sprachschatz von OWL2 RL angepasst. Was aber besonders zu erwähnen ist sind die Relationen \emph{list} und \emph{history} \ref{relations-list-history}. Sie speichern keine Axiome wie alle anderen. Die Relation list dient als Hilfsstruktur für Axiome, die eine variable Anzahl von Elementen abspeichern. Die Relation history wird benutzt, um den Abhängigkeitsverlauf beim inferieren von Fakten speichern zu können. Damit ist es möglich Ableitungen gezielt wieder rückgängig zu machen. Deswegen sind auch alle anderen Relationen mit einer \emph{id} Spalte ausgestattet, die Axiome bzw. komplexe Unterausdrücke eindeutig identifiziert.

Allgemein wurde bei der Erstellung der Tabellenstruktur darauf geachtet, das die Formulierung der Regeln in SQL vereinfacht wird. So ist im Normalfall die Ergebnismenge einer Regel dadurch zu erhalten, in dem man die beteiligten Relationen auf den Variablen, die in den Regeln angegeben sind, miteinander joint.

\input{db-schema}

\subsection{Optimierungen für das MEMA-Prinzip}

Im Paper Classifying $\mathcal{ELH}$ Ontologies in SQL Databases \cite{Delaitre2009} befinden sich interessante Ansätze für Verbesserungen.

Zum einen wird eine Normalisierung vorgeschlagen. Diese ist über zwei Wege möglich:
\begin{itemize}
  \item Vereinheitlichung von strukturell gleichen Ausdrücken. Das bedeutet, das wenn die selben Ausdrücke mehrmals in einer Ontologie vorkommen, z.B. als Teile in einem komplexeren Ausdruck, werden alle diese gleichen Unterausdrücke nur einmal abgespeichert. Das hat den Vorteil, dass es den Speicherplatz verringert und über den selben Ausdruck nicht mehrmals geschlossen wird. Dies ist mit der OWLAPI sogar relativ leicht möglich, da sie schon von selbst strukturelle gleiche Ausdrücke zusammenfasst. Allerdings wurde diese Optimierung nicht umgesetzt, den für die Möglichkeit abgeleitete Fakten wieder zu löschen muss der Weg wie Fakten entstanden sind eindeutig sein. Hier würde man aber Probleme verursachen. Diese Optimierung ist grundsätzlich nicht ausgeschlossen. Für den Fall, dass man nichts löschen müsste könnte sie direkt umgesetzt werden. Für den Fall das man etwas löschen will, müsste man etwas geschickter Vorgehen. Auf diese Optimierung wurde auf Grund ihrer Komplexität in diesem Fall verzichtet.
  \item Umwandlung gewisser Konstrukte. Es wird außerdem vorgeschlagen manche Konstrukte umzuwandeln, so z.B. $A \sqsubseteq B \land C \Rightarrow A \sqsubseteq B, A \sqsubseteq C$. Damit könnte man sich manche Regeln sparen. Inwiefern dies im RL Fragment nützlich ist, da hier doch recht viele Konstrukte aus OWL2 zu gelassen sind und welche Regeln man damit wirklich einsparen kann ist eine offene Frage.
\end{itemize}


Des Weiteren wird beschrieben, wie sie die Leistung beim Laden der Ontologie steigern. Hier werden ebenfalls zwei Tricks verwendet:
\begin{itemize}
  \item Zum einen wird die Ontologie \emph{on-the-fly} in die Datenbank geschrieben, d.h. während die Ontologie gelesen wird, wird sie auch parallel in die Datenbank geschrieben. Das hat den Vorteil das sie nicht zweimal durchlaufen werden muss. Die OWLAPI lässt zwar auch die Serialisierung direkt in eine Datenbank zu. Das Laden der Ontologie in die OWLAPI und den Schlussfolgerer ist aus den folgenden Gründen entkoppelt. Zum einen kann es sein, dass man die OWLAPI erst einmal lädt und erst später den Schlussfolgerer dazu startet. So ist zumindest der normale Ablauf. Hier würde ein solche Optimierung also nichts bringen. Zweitens müsste man sich bei einer direkten Serialisierung um alle Konstrukte in einer Ontologie kümmern und nicht nur um die, die man zum ableiten verwendet. Es würde also den Schlussfolgerer komplexer in seinem Aufbau machen.
  \item Es werden Konstrukte immer in eine \emph{in-memory} Relation eingefügt und dann in größeren Blöcken auf einmal auf eine Festplatte geschrieben. Ob diese Optimierung wirklich soviel bringt ist fraglich, da ein gutes Datenbanksystem ein solches Caching von alleine betreiben sollte. Außerdem liegt der Overhead dabei vermutlich eher beim parsen der vielen INSERT Statements. Trotzdem könnte man diese Optimierung in Betracht ziehen. Sie wurde ebenfalls aus Komplexitätsgrunden außer Acht gelassen.
\end{itemize}

\subsection{Andere Serialisierungsmöglichkeiten}
Die OWLAPI bietet eine Reihe von Formaten zur Serialisierung von OWL-Ontologien an. Diese sind zum Teil auch in der Spezifikation von OWL erwähnt. Das Problem was allen Formaten gemein ist, ist das es einfach Textformate sind die entweder vollständig in den Hauptspeicher eingelesen werden müssen oder nur sehr langsame Zugriffsmöglichkeiten anbieten.

Dieses Problem wurde schon von mehreren erkannt \cite{Kleb2009ProtegeDB}, \cite{Kleb2009OWLDB} und versucht durch eine Abbildung in einer Datenbank zu lösen.

Hier ist das Problem einerseits, dass sich dieses Paper noch mit der OWLAPIv2 beschäftigen und andererseits ist es ja nicht das Ziel dieser Diplomarbeit  eine Serialisierungsmöglichkeit für eine komplette Ontologie in einer Datenbank zu finden, sondern nur die nötigen Fakten abzulegen, um darauf gut Regeln in SQL abbilden zu können.